---
title: "RegressionProject"
author: "Rohith Anugolu"
date: "`r format(Sys.Date(), '%A, %B %d, %Y')`"
output:
  html_document: default
  pdf_document: default
---

**Research question: **
**Can carat weight of a diamond be determined from visually observable characteristics**
**i.e. without actually weighing the diamond. Note: Price is not observable, so do not use price.**
**Come up with a way to ascertain the carat weight of a diamond without physically touching it. All other variables in the dataset can be determined visually.**

**Load necessary packages**

```{r loading-lib, warning=FALSE,message=FALSE}
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(mosaic)
library(GGally)
library(caret)
```


```{r}
# Let's make a residual.analysis function to save typing.
residualAnalysis <- function(model=NULL) {
if(!require(gridExtra)) stop('Install the gridExtra package.')
if(!require(ggformula)) stop('Install the ggformula package.')
df <- data.frame(Prediction=predict(model$fit),
Residual=rstandard(model$fit))
p1 <- gf_point(Residual~Prediction,data=df) %>% gf_hline(yintercept = 0)
p2 <- gf_qq(~Residual,data=df) %>% gf_qqline()
grid.arrange(p1, p2, ncol=2)
}
```


```{r}
head(diamonds)
```

**We are going to check the quantitative variables of the data set**

```{r}
quant <- sapply(diamonds, is.numeric)
names(diamonds)[quant]
```

**Weight of a carat is closely related to the volume so we try to create `volume` feature. First we check the pairwise correlation between `carat`,`x`,`y`,`z`**

```{r vol-rlshp}
diamonds %>% 
  select(carat,x,y,z) %>% 
  ggpairs()
```

**`x`,`y`,`z` have high correlation with `carat` with the first look it is a good indication but we will confirm further if it is a good measure.**

**Question: Can volume be used as a good predictor for carat?** 
Let us now calculate the volume 

```{r volume}
diamonds %>%
  mutate(volume = x*y*z) -> diamonds

head(diamonds)

```

**Visualizing the relationship between `carat` and `volume`**

```{r}
diamonds %>% 
  select(carat,x,y,z, depth,table) %>% 
  ggpairs()
```

**`depth` & `table` have a weak correlation with carat. They will not be used in the model**

```{r cut-pred}
diamonds <- diamonds %>%
  mutate(
    quality_cut = if_else(cut %in% c("Ideal", "Premium"), "High", "Low"),
    quality_color = if_else(color %in% c("D", "E", "F"), "High", "Low"),
    quality_clarity = if_else(clarity %in% c("IF", "VVS1", "VVS2"), "High", "Low")
  )
diamonds <- diamonds %>%
  mutate(
    quality_cut = as.factor(quality_cut),
    quality_color = as.factor(quality_color),
    quality_clarity = as.factor(quality_clarity)
  )
head(diamonds)
```

**Visualizing the relationships of the created predictors**

```{r}
diamonds %>%
  select(carat, volume, quality_cut, quality_color, quality_clarity) %>%
ggpairs()
```

## Train-Test Split

**Spliting the data into training and test sets**

```{r}
set.seed(2025)
diamonds_split <- createDataPartition(diamonds$carat, p = 0.80, list = FALSE)

train_data <- diamonds[diamonds_split,]
test_data <- diamonds[-diamonds_split,]

train_data
test_data
```

## Multiple Linear Regression

### Using backward selection to identify predictors contributing to the model

```{r backward}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

back_selection <- lm_spec |>
    fit(carat ~ volume + quality_cut + quality_color + quality_clarity, 
                data = train_data)

tidy(back_selection$fit)
```

```{r}
glance(back_selection$fit)
```

### Multiple Linear Regression (reduced model)

Creating lm_model 

```{r lm-model}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

mlr_mod <- lm_spec %>%
  fit(carat ~ volume * quality_cut + quality_clarity, data = train_data)

tidy(mlr_mod)
```

### Check the assumptions

```{r}
residualAnalysis(mlr_mod)
```


```{r}
plot(mlr_mod$fit)
```

### Confidence Intervals

```{r}
confint(mlr_mod$fit, level = 0.95)

```


```{r pred}
predictions <- augment(mlr_mod, new_data = test_data)
predictions

```

### Model fit

Evaluating model perfomance on the test data

```{r}
model_summary <- glance(mlr_mod)
print(model_summary)
```

### Confidence & Prediction Intervals on test data

```{r fig.align='left'}
pred_intervals <- cbind(
  test_data,
  predict(mlr_mod, new_data=test_data),
  predict(mlr_mod, new_data=test_data, type="conf_int"),
  predict(mlr_mod, new_data=test_data, type="pred_int"))

colnames(pred_intervals) <- make.unique(names(pred_intervals))

pred_intervals <- pred_intervals |>
  rename(.conf_lower = .pred_lower, .conf_upper = .pred_upper, .pred_lower = .pred_lower.1, .pred_upper = .pred_upper.1)
```


```{r}
pred_intervals |>
  gf_point(carat ~ volume, col = "grey") |>
  gf_line(.pred ~ volume) |>
  gf_line(.conf_lower ~ volume, col='red') %>%
  gf_line(.conf_upper ~ volume, col='red') %>%
  gf_line(.pred_lower ~ volume, col='blue') %>%
  gf_line(.pred_upper ~ volume, col='blue') +
  facet_wrap(~ quality_cut + quality_clarity) +
  gf_theme(theme_classic()) +
  labs(title = "Confidence and prediction intervals", x = "Magnitude", y = "Carat") 
```

## Polynomial Regression

**Data to use for the model**

```{r}
polData <- diamonds %>%
  select(carat,table,depth,x,y,z)
```

### Identifying skewness through Histogram

```{r}
library(gridExtra)
dPlot <- ggplot(polData, aes(x = depth)) + geom_histogram(bins = 50, fill="blue", alpha=0.5) + ggtitle("Depth")
tPlot <- ggplot(polData, aes(x = table)) + geom_histogram(bins = 50, fill="red", alpha=0.5) + ggtitle("Table")
xPlot <- ggplot(polData, aes(x = x)) + geom_histogram(bins = 50, fill="green", alpha=0.5) + ggtitle("X")
yPlot <- ggplot(polData, aes(x = y)) + geom_histogram(bins = 50, fill="purple", alpha=0.5) + ggtitle("Y")
zPlot <- ggplot(polData, aes(x = z)) + geom_histogram(bins = 50, fill="orange", alpha=0.5) + ggtitle("Z")

grid.arrange(dPlot, tPlot, xPlot, yPlot, zPlot, ncol=3)
```

**We are implementing log transformation to check the impact to the model perfomance**

**`depth` and `table` have an appproximate normal distribution so there is no need for log transformation. `x` has slight skewed and log transformation might help,`y`,`z` are strong right-skewed so we implement log transformation.**

```{r}
polData <- diamonds %>%
  mutate(
    log_x = log(x+1),
    log_y = log(y+1),
    log_z = log(z+1)
  )
```

```{r}
skimr::skim(polData) |>
  select(-numeric.hist)
```

### Train - Test Split

```{r}
set.seed(2025)
library(caret)
trainIndex <- createDataPartition(polData$carat, p = 0.8, list = FALSE)
trainData <- polData[trainIndex, ]
testData  <- polData[-trainIndex, ]
```

**Let us fit a polynomial regression model**

```{r}
polyModel <- lm(carat ~ poly(x, 2, raw = TRUE) +
                  poly(y, 2, raw = TRUE) +
                  poly(z, 2, raw = TRUE) +
                  depth + table,
                data = trainData)

summary(polyModel)
```


### Making Predictions on test data

```{r}
testData$predicted <- predict(polyModel, testData)
```

```{r}
rmse <- sqrt(mean((testData$predicted - testData$carat)^2))
print(paste("Test RMSE:", round(rmse, 4)))
```

**The Root Mean Squared Error on the test data is smaller than Residual Standard Error which shows there is no overfitting in the model.**

### Confidence Intervals

```{r}
confint(polyModel, level = 0.95)
```


### Check the assumptions

```{r}
plot(polyModel)
```
